{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cnn trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = mnist.train_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8] (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=mnist.test_images()\n",
    "test_labels=mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255\n"
     ]
    }
   ],
   "source": [
    "print(train_images.min(), train_images.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "#Normalize images from 0-255 to [-0.5,0.5]\n",
    "# makes the network easier to train, when values are :\n",
    "# smaller and centered > this usually leads to better results.\n",
    "\n",
    "train_images = (train_images/255) - 0.5\n",
    "test_images = (test_images/255) - 0.5\n",
    "\n",
    "print(train_images.min(), train_images.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the images from (28,28) to (28,28,1) as Keras requires the thrid dimension\n",
    "# 1 = gray image, 3 = RGB image n = higher dimension image\n",
    "\n",
    "# np.expand_dims() adds an axis\n",
    "\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "print(train_images.shape, test_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN part of the code\n",
    "# we will use Sequential model, that presents linear stack of layers \n",
    "# output goes to input, no cyckling in the models (such as LSTM is impossible with this model)\n",
    "\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model/constructor takes an array of Keras layers.\n",
    "# We use 3 layers: convolutional, max pooling and softmax\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "# call model and pass an instance to it\n",
    "#define parameters for model\n",
    "\n",
    "# padding='same' means adding 0's around the image producing same sized filters\n",
    "# as the image fed to the network\n",
    "# adding second conv2d layer increased the accuracy by from 0.96 to 0.97\n",
    "# adding dropout: prevents overfitting= learning test set well, not understanding\n",
    "# test set. rate = fraction of the input units to drop 0.971\n",
    "# adding dense(64..) i.e. fully connected layer after convolutional layer and\n",
    "# before output layer ( a typical CNN structure). units = number of nodes 0.979\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "   Conv2D(filters=num_filters, kernel_size=filter_size, padding='same',input_shape=(28,28,1)),\n",
    "    Conv2D(num_filters, filter_size, padding='same'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Dropout(rate = 0.2),\n",
    "    Flatten(),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=10, activation='softmax')    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                100416    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 101,730\n",
      "Trainable params: 101,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model i.e, configurint the training process\n",
    "# what optimizer, loss functions and metrics are used\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8] [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# training labels for the images fed to the network\n",
    "train_labels = mnist.train_labels()\n",
    "print(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]] (60000, 10) [[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# our labels are a list of integers, where each number represents the class for one image.\n",
    "# this needs to be changed to categorical form by using one-hot vectors.\n",
    "# this way we can feed the data to Keras, as it expects these labels to be categorical vectors.\n",
    "# i.e. instead saying image_0's label is 5, way say that it is [0,0,0,0,0,1,0,0,0,0], in one-hot vector\n",
    "# This representation we can feed to keras model without errors.\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "print(train_labels, train_labels.shape, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - 20s 421us/step - loss: 0.2602 - acc: 0.9199 - val_loss: 0.1077 - val_acc: 0.9662\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - 20s 416us/step - loss: 0.1057 - acc: 0.9669 - val_loss: 0.0818 - val_acc: 0.9756\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - 19s 403us/step - loss: 0.0760 - acc: 0.9759 - val_loss: 0.0711 - val_acc: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15b31c5b400>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model, by calling fit() -function and specifying parameters: training data, \n",
    "# number of epochs (i.e. iterations over the entire dataset), validation data\n",
    "\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs = 3,\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package to save model weigths in .h5 format.\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model weights, ready-trained model, so that it can be used again anytime\n",
    "\n",
    "model.save_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# reload the trained model \n",
    "weights = model.load_weights('cnn.h5')\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.6867338e-06 1.4287612e-08 1.2044975e-05 1.5760382e-04 1.0440846e-06\n",
      "  1.0789305e-07 4.6235227e-11 9.9982482e-01 4.9584223e-08 1.6443012e-06]\n",
      " [3.2097840e-04 9.4206611e-05 9.9547487e-01 2.8281490e-04 8.5492458e-09\n",
      "  1.5680055e-04 3.4933856e-03 7.9347570e-12 1.7694257e-04 1.7613157e-08]\n",
      " [2.1520598e-05 9.9520314e-01 1.9347651e-03 1.6155311e-04 4.5565888e-04\n",
      "  9.5155032e-05 1.6669280e-04 1.7921421e-03 1.3775448e-04 3.1694803e-05]\n",
      " [9.9768305e-01 2.1226189e-08 1.0001265e-03 1.2441712e-05 4.7176195e-06\n",
      "  2.8484213e-04 8.3691417e-04 1.5011485e-04 1.2694619e-05 1.5007169e-05]\n",
      " [6.5808679e-04 6.5386685e-06 2.2377414e-03 8.7697217e-06 9.8572111e-01\n",
      "  1.1097582e-05 8.0070563e-04 8.0966437e-03 1.4100330e-04 2.3184104e-03]]\n"
     ]
    }
   ],
   "source": [
    "# using the trained model to make predictions by passing an array of inputs to predict()\n",
    "# that returns an array of outputs.\n",
    "\n",
    "prediction = model.predict(test_images[:5])\n",
    "\n",
    "# print the prediction for each 10 classes\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# np.argmax() returns the maximum value from a list/ axis wanted\n",
    "# i.e. as we have ten classes, each of them have a number of prediction\n",
    "# when we use the argmax, it returns the column number that has the highest \n",
    "# number, i.e. the best guess of the network of what the output might be.\n",
    "# axis=1 means it will take each list from list and return its highest value,\n",
    "# not the whole list of lists highest value (only 1 number)\n",
    "\n",
    "print(np.argmax(prediction, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(test_labels[:5], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
